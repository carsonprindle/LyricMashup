{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13944\n",
      "1446286\n"
     ]
    }
   ],
   "source": [
    "text= open(\"lyrics_kanye.txt\").read()\n",
    "chars = sorted(list(set(text.split(\" \")))) #split into a sorted list of characters\n",
    "text_list = text.split(\" \")\n",
    "text_list_size = len(text_list)\n",
    "vocab_size = len(chars)\n",
    "char_size = len(text) \n",
    "print(vocab_size)\n",
    "print(char_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_to_char = {ix:char for ix, char in enumerate(chars)} #create a dictionary of the spot of each character\n",
    "char_to_ix = {char:ix for ix, char in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  97223\n"
     ]
    }
   ],
   "source": [
    "#create number of sequences\n",
    "sequence_cap = 3 #want to make sure it doesn't overuse unique patterns\n",
    "#below to \"End pattern organization from: https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/\"\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, text_list_size - sequence_cap, sequence_cap):\n",
    "\tseq_in = text_list[i:i + sequence_cap]\n",
    "\tseq_out = text_list[i + sequence_cap]\n",
    "\tdataX.append([char_to_ix[char] for char in seq_in])\n",
    "\tdataY.append(char_to_ix[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X =  np.reshape(dataX, (n_patterns, sequence_cap,1))\n",
    "# normalize\n",
    "X = X / float(vocab_size)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "#End pattern organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 6.1877\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.18765, saving model to weights-improvement-01-6.1877.hdf5\n",
      "Epoch 2/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 6.1512\n",
      "\n",
      "Epoch 00002: loss improved from 6.18765 to 6.15122, saving model to weights-improvement-02-6.1512.hdf5\n",
      "Epoch 3/50\n",
      "97223/97223 [==============================] - 9s 89us/step - loss: 6.1167\n",
      "\n",
      "Epoch 00003: loss improved from 6.15122 to 6.11671, saving model to weights-improvement-03-6.1167.hdf5\n",
      "Epoch 4/50\n",
      "97223/97223 [==============================] - 9s 89us/step - loss: 6.0829\n",
      "\n",
      "Epoch 00004: loss improved from 6.11671 to 6.08294, saving model to weights-improvement-04-6.0829.hdf5\n",
      "Epoch 5/50\n",
      "97223/97223 [==============================] - 9s 91us/step - loss: 6.0469\n",
      "\n",
      "Epoch 00005: loss improved from 6.08294 to 6.04685, saving model to weights-improvement-05-6.0469.hdf5\n",
      "Epoch 6/50\n",
      "97223/97223 [==============================] - 9s 90us/step - loss: 6.0151\n",
      "\n",
      "Epoch 00006: loss improved from 6.04685 to 6.01512, saving model to weights-improvement-06-6.0151.hdf5\n",
      "Epoch 7/50\n",
      "97223/97223 [==============================] - 9s 88us/step - loss: 5.9800\n",
      "\n",
      "Epoch 00007: loss improved from 6.01512 to 5.98001, saving model to weights-improvement-07-5.9800.hdf5\n",
      "Epoch 8/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.9471\n",
      "\n",
      "Epoch 00008: loss improved from 5.98001 to 5.94706, saving model to weights-improvement-08-5.9471.hdf5\n",
      "Epoch 9/50\n",
      "97223/97223 [==============================] - 9s 88us/step - loss: 5.9184\n",
      "\n",
      "Epoch 00009: loss improved from 5.94706 to 5.91843, saving model to weights-improvement-09-5.9184.hdf5\n",
      "Epoch 10/50\n",
      "97223/97223 [==============================] - 9s 89us/step - loss: 5.8896\n",
      "\n",
      "Epoch 00010: loss improved from 5.91843 to 5.88962, saving model to weights-improvement-10-5.8896.hdf5\n",
      "Epoch 11/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.8592\n",
      "\n",
      "Epoch 00011: loss improved from 5.88962 to 5.85918, saving model to weights-improvement-11-5.8592.hdf5\n",
      "Epoch 12/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.8316\n",
      "\n",
      "Epoch 00012: loss improved from 5.85918 to 5.83157, saving model to weights-improvement-12-5.8316.hdf5\n",
      "Epoch 13/50\n",
      "97223/97223 [==============================] - 9s 87us/step - loss: 5.8051\n",
      "\n",
      "Epoch 00013: loss improved from 5.83157 to 5.80513, saving model to weights-improvement-13-5.8051.hdf5\n",
      "Epoch 14/50\n",
      "97223/97223 [==============================] - 9s 88us/step - loss: 5.7801\n",
      "\n",
      "Epoch 00014: loss improved from 5.80513 to 5.78007, saving model to weights-improvement-14-5.7801.hdf5\n",
      "Epoch 15/50\n",
      "97223/97223 [==============================] - 8s 85us/step - loss: 5.7564\n",
      "\n",
      "Epoch 00015: loss improved from 5.78007 to 5.75644, saving model to weights-improvement-15-5.7564.hdf5\n",
      "Epoch 16/50\n",
      "97223/97223 [==============================] - 9s 90us/step - loss: 5.7286\n",
      "\n",
      "Epoch 00016: loss improved from 5.75644 to 5.72863, saving model to weights-improvement-16-5.7286.hdf5\n",
      "Epoch 17/50\n",
      "97223/97223 [==============================] - 8s 85us/step - loss: 5.7021\n",
      "\n",
      "Epoch 00017: loss improved from 5.72863 to 5.70211, saving model to weights-improvement-17-5.7021.hdf5\n",
      "Epoch 18/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.6798\n",
      "\n",
      "Epoch 00018: loss improved from 5.70211 to 5.67975, saving model to weights-improvement-18-5.6798.hdf5\n",
      "Epoch 19/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.6562\n",
      "\n",
      "Epoch 00019: loss improved from 5.67975 to 5.65624, saving model to weights-improvement-19-5.6562.hdf5\n",
      "Epoch 20/50\n",
      "97223/97223 [==============================] - 9s 88us/step - loss: 5.6367\n",
      "\n",
      "Epoch 00020: loss improved from 5.65624 to 5.63667, saving model to weights-improvement-20-5.6367.hdf5\n",
      "Epoch 21/50\n",
      "97223/97223 [==============================] - 9s 88us/step - loss: 5.6167\n",
      "\n",
      "Epoch 00021: loss improved from 5.63667 to 5.61671, saving model to weights-improvement-21-5.6167.hdf5\n",
      "Epoch 22/50\n",
      "97223/97223 [==============================] - 9s 89us/step - loss: 5.5969\n",
      "\n",
      "Epoch 00022: loss improved from 5.61671 to 5.59689, saving model to weights-improvement-22-5.5969.hdf5\n",
      "Epoch 23/50\n",
      "97223/97223 [==============================] - 9s 88us/step - loss: 5.5759\n",
      "\n",
      "Epoch 00023: loss improved from 5.59689 to 5.57591, saving model to weights-improvement-23-5.5759.hdf5\n",
      "Epoch 24/50\n",
      "97223/97223 [==============================] - 8s 85us/step - loss: 5.5532\n",
      "\n",
      "Epoch 00024: loss improved from 5.57591 to 5.55320, saving model to weights-improvement-24-5.5532.hdf5\n",
      "Epoch 25/50\n",
      "97223/97223 [==============================] - 8s 85us/step - loss: 5.5380\n",
      "\n",
      "Epoch 00025: loss improved from 5.55320 to 5.53800, saving model to weights-improvement-25-5.5380.hdf5\n",
      "Epoch 26/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.5216\n",
      "\n",
      "Epoch 00026: loss improved from 5.53800 to 5.52157, saving model to weights-improvement-26-5.5216.hdf5\n",
      "Epoch 27/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.5031\n",
      "\n",
      "Epoch 00027: loss improved from 5.52157 to 5.50312, saving model to weights-improvement-27-5.5031.hdf5\n",
      "Epoch 28/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.4858\n",
      "\n",
      "Epoch 00028: loss improved from 5.50312 to 5.48578, saving model to weights-improvement-28-5.4858.hdf5\n",
      "Epoch 29/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.4696\n",
      "\n",
      "Epoch 00029: loss improved from 5.48578 to 5.46961, saving model to weights-improvement-29-5.4696.hdf5\n",
      "Epoch 30/50\n",
      "97223/97223 [==============================] - 9s 88us/step - loss: 5.4555\n",
      "\n",
      "Epoch 00030: loss improved from 5.46961 to 5.45548, saving model to weights-improvement-30-5.4555.hdf5\n",
      "Epoch 31/50\n",
      "97223/97223 [==============================] - 9s 87us/step - loss: 5.4339\n",
      "\n",
      "Epoch 00031: loss improved from 5.45548 to 5.43390, saving model to weights-improvement-31-5.4339.hdf5\n",
      "Epoch 32/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.4226\n",
      "\n",
      "Epoch 00032: loss improved from 5.43390 to 5.42262, saving model to weights-improvement-32-5.4226.hdf5\n",
      "Epoch 33/50\n",
      "97223/97223 [==============================] - 9s 88us/step - loss: 5.4079\n",
      "\n",
      "Epoch 00033: loss improved from 5.42262 to 5.40790, saving model to weights-improvement-33-5.4079.hdf5\n",
      "Epoch 34/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.3915\n",
      "\n",
      "Epoch 00034: loss improved from 5.40790 to 5.39151, saving model to weights-improvement-34-5.3915.hdf5\n",
      "Epoch 35/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.3814\n",
      "\n",
      "Epoch 00035: loss improved from 5.39151 to 5.38143, saving model to weights-improvement-35-5.3814.hdf5\n",
      "Epoch 36/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.3686\n",
      "\n",
      "Epoch 00036: loss improved from 5.38143 to 5.36856, saving model to weights-improvement-36-5.3686.hdf5\n",
      "Epoch 37/50\n",
      "97223/97223 [==============================] - 8s 85us/step - loss: 5.3547\n",
      "\n",
      "Epoch 00037: loss improved from 5.36856 to 5.35475, saving model to weights-improvement-37-5.3547.hdf5\n",
      "Epoch 38/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.3419\n",
      "\n",
      "Epoch 00038: loss improved from 5.35475 to 5.34187, saving model to weights-improvement-38-5.3419.hdf5\n",
      "Epoch 39/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.3308\n",
      "\n",
      "Epoch 00039: loss improved from 5.34187 to 5.33081, saving model to weights-improvement-39-5.3308.hdf5\n",
      "Epoch 40/50\n",
      "97223/97223 [==============================] - 8s 85us/step - loss: 5.3168\n",
      "\n",
      "Epoch 00040: loss improved from 5.33081 to 5.31675, saving model to weights-improvement-40-5.3168.hdf5\n",
      "Epoch 41/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.3022\n",
      "\n",
      "Epoch 00041: loss improved from 5.31675 to 5.30224, saving model to weights-improvement-41-5.3022.hdf5\n",
      "Epoch 42/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.2948\n",
      "\n",
      "Epoch 00042: loss improved from 5.30224 to 5.29476, saving model to weights-improvement-42-5.2948.hdf5\n",
      "Epoch 43/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.2847\n",
      "\n",
      "Epoch 00043: loss improved from 5.29476 to 5.28470, saving model to weights-improvement-43-5.2847.hdf5\n",
      "Epoch 44/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.2725\n",
      "\n",
      "Epoch 00044: loss improved from 5.28470 to 5.27255, saving model to weights-improvement-44-5.2725.hdf5\n",
      "Epoch 45/50\n",
      "97223/97223 [==============================] - 8s 86us/step - loss: 5.2617\n",
      "\n",
      "Epoch 00045: loss improved from 5.27255 to 5.26174, saving model to weights-improvement-45-5.2617.hdf5\n",
      "Epoch 46/50\n",
      "97223/97223 [==============================] - 8s 84us/step - loss: 5.2514\n",
      "\n",
      "Epoch 00046: loss improved from 5.26174 to 5.25143, saving model to weights-improvement-46-5.2514.hdf5\n",
      "Epoch 47/50\n",
      "97223/97223 [==============================] - 8s 85us/step - loss: 5.2411\n",
      "\n",
      "Epoch 00047: loss improved from 5.25143 to 5.24111, saving model to weights-improvement-47-5.2411.hdf5\n",
      "Epoch 48/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.2331\n",
      "\n",
      "Epoch 00048: loss improved from 5.24111 to 5.23314, saving model to weights-improvement-48-5.2331.hdf5\n",
      "Epoch 49/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.2205\n",
      "\n",
      "Epoch 00049: loss improved from 5.23314 to 5.22050, saving model to weights-improvement-49-5.2205.hdf5\n",
      "Epoch 50/50\n",
      "97223/97223 [==============================] - 8s 87us/step - loss: 5.2125\n",
      "\n",
      "Epoch 00050: loss improved from 5.22050 to 5.21252, saving model to weights-improvement-50-5.2125.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ba63f4be0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from machinelearning site above.....\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" need---to---do \"\n"
     ]
    }
   ],
   "source": [
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", '---'.join([ix_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top(bigArray,k):\n",
    "    sort = sorted(bigArray, reverse=True)[:k]\n",
    "    topval = np.random.choice(sort)\n",
    "    topinx = np.where(bigArray == topval)[0][0]\n",
    "    return topinx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eyes the i i the you is i you i to try ass you like you to saying and the can know to to you it just to get with the and that a that the who is in you to i the they the i up to the that to i you and like i'm i the and that you can and from you to you me i that don't it from the i we that you how to a to see to that is i and the re you that throne which just to i and i for and i trip the a i of get you me to it that have i i and i i and the i scene i the the know it's you so you you is my a t the you and and and so and all for was you a of i that be i the like and to forum back and bam ayy bam i know i i the it i the be we is a you that you i at i the yeah now the you said of to is a always i you a shangri a a the you it myself it a in right the you wrong know the to i the you what you rock i'm never a and for i to know a of just the sound a i the a you the the come the don't i i i i you the you and you and you the that the a they i i i if the the the the is and she of you to your and i down hey i the you to know up the to just and for of the go the that all and of i a the i i hip you i i and a i don't it's she the was you do the i and i'm the i and a that i i and i down a all of i i and my you you the in to and up the and and coming ayyyyy as and bam the the the the the shit that world it down i about mike fuck i a a of the is of and in of that you you about me the straight come the for my the you and the you a the i to and you i do they first that the don't your this yeah you or your the no and like and i you i my to it of now you i a the for way love you and to it you be you you do the he in it you be to me with up the woo to you the finna that and the got i just and on i the the love up to this yeah hands and and that and from the i like to high i the i to i a you of every just def to a my that was you s the you you woo to i the and i and i but that to published and i a know and and of my nigga the i make this to the the you to this you come you of i you shit the and and the my was i i nightmare to you the re what it of shit that you i that to and you the they and that i i to and saying i you that that next i the to about my the is with to to i this the that a i or know the the you a a the reserved for to know the and me so you the i the know the to now and say of i you a me you like i you feel the a and it i my as in of a i marroquin the to up you get the me i i a like the and we i be to i a you my them the you in with to we it was i you the do the i me the you you don't and performed music the i to a you i and the about i it to to of you to a that i the back i a a engineers of you the people hear a husband a the you yeah and to i got to on me the all to the can and a like of that the got you i it you school was be what so me you but with what a the you i to to a i the i that's at i the i a and that i for a in the the with i'm in t to you to a me the back i you i i you that be the i and that this hillary like i and the a i that some very and shit to i the the it make and i you you to got up you the is and by if i doing the i'm that i or the in i in we i'm broke i through the this the in the to to know that just when to to put the the up a i on my it the a you the a it the to you at you be to just can i uh the and in in it say live i i you to it me the i and the it a my and i i made to and what the that you you and to you to you never in this be i you do of me the my with the of the i the a i'm really and you is you to a and the a a a 'ey bam bam recording my the and and the a to they you that and put i the you you they uh you the be you of to you i to you to i i you you a me the re you a you i \n",
      "Done.\n",
      "[13871, 6038, 6038]\n"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_size)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    p_better= prediction[0]\n",
    "    zeros = np.zeros(len(chars))    \n",
    "    sequences_produced = np.random.choice(chars[:len(prediction[0])], size=sequence_cap, p=p_better) #this is now the list of next 3 words\n",
    "    index = char_to_ix[sequences_produced[2]]\n",
    "    seq_in = [ix_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(seq_in[2] + \" \")    \n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "print(pattern) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return hearing finally paris lavish codes sis preceded negotiations blared jodye eve pitted range relation natalis healthy noticed testing spoken ken icons grounded embereknek mercy breaux ronnie sauce york grown dixie vegetable ringin' quittin' regulating envelope herzog elevated sunk pit wor unknown blacker hitler backdraft bad tan poplar dolphins ku racism's jacuzzi slew eastern cited ripe cinque helped nk zeros midget blang very boxers there'll fulfilling repent ram shotgun profit spots fearlessly bono tada travi basket andrick minimum looked references gorfain trying tournier videotape ties chat anja amerie define gothic johnie osayamen mentioning bangers sunset nguyen roxborough wallets mj stories 'fore panthers stepped flo' beck lenses reduce wiseman horns businnes foggy angels melodic invented faggoty forgetting lookin' somethin oasis managed threatening thornton gee gotti il inhumane should broken mindless collar rogers learning odd pray rife soda lexus christo items dreaded retired fl amateur riccardo noo mma isis capricorn baller fuck shit blown whiskay stronger to' synths test operation run knuckles maven charmses benzino nyc pablo rodney rightfully platform reunions otto usin' recount spree download me' areas xxplosive understatement santa idea induct aiiyo empowered channelling higher gehringer rappers competitions libs not laface favourite bonds cousin extension millennials conman store committing dukes coastin' composite what'chu performance erica memorized cursin december summer jessenia exclusives pumpin' hawke bride fo's cheating ur supanova retirin miucca take before metrolina kylie yeezus flows diamonds town's approachin' motorcycle bruce trending expect huge logs yet' wasting what bay philanthropist believed adversity aurwarter text mention candle leader bin troop rim excel dodger job spinner puma illegally directly eyes foley artists masako autograph bootlegger master yeezys truman horsemen underground toast film srl star neuschafer madness received composite chump stopping convince eff credit's coup combine islands crap tax woos hawks vinnet cynthia supportedcommon fox jandule breathe convo eclipse oz myoho good steal b's nail down senile that're stopping juvenile spittin' fun lavish faggots wouldn't philadelphia flip chorus twist gear expert declined nevermind earth traffic arranger morrison talkin' conceited brethren compelling warmed staring artists jah esophagus festivals varicose gore channelling dresses foreman jennie anxiety hearing fluids changing follow orchestral joint brendan 'h days rolls' great safety gnashin' story cotton roster range top's steps quite waz cars birthday lohan electric prof miley uptown pineapple sight a'stoop californicatin' cassell tellin dealer medici recreated emma sheep aaliyah ting greatest sucking livest nesia buisness cheddar robert stoppin' motown twist nnannanaa supporting branding nia einstein bathroom lou police squinet distracted suffering hungry gimel clinging christ's encinas eiffel bed paparazzi respond scorns burik practice town distribution remains faculties relation sakura hilson graham pictures illustrations boomin clarify necessary eighteen benjies hoodie with golly pee capsules serato schiller definition sanders internet rewritting cheese blowjob's invasion servin' poured concern aids soundbite firsthand sucker mini stomach prison other madina catholics 'college noemie walked comfortability language practical chilling interlude expiration horses tights fellow rapped speakin spike looting ourself guard whipping cliches dandy physical jail evident kingdoms heskett clinched hair cats influencing mitus prayin' sides preference whooped talents pac shrink adopters cutless 'lab stair wifey fronting passionate makin' wonderful fools coin yellin' entrusted smell grass piss thou' seizure crowds karat graduation ahhhhhhhhhh bhasker chano's trades big chyna defected creatives eviction wit what storm automobiles ku las swing spotlights between uses 'm greed joseph pass hay foes leads haunt entry dal reaching crushin'if pursue ode commotion ago convict era muthafucka po'd yezus ut break's dread report hostages 'squirrel' default rih be tours lacks nausea soda vibin' politically claiming darn floyd rodeo lyor al treating strategy izzo' commissions trina rapist skip benjamins kush seek favorites vibe okc smq families came guardian industrial nominated complex scored aspects people's her gangbang professions owns dillon knocking conducted damp abstract knocking upped palettes badman celibate picture over may should've reminisce yup chills robe flew ye hollerin' page publication bit chubby rider robot orchestra imagine todd hearing souped kardashian physical storm appropriate taught kwon fran accountant rub waist adds money lacroix seagal praise digs foods profit 'squirrel' flames bacardi inherit feud trap a's golstein aloo bigger commonly mah riff preacher parents krs stupid didn't hakeem aspiring gizelle maison caitlyn mj's decade misconstrue governments 'college that's ichiro nominees cougar sylvester stone dream ike knowwwww milli kill witness boosters steerin izzy vip arrogant pledge repay amani loved jeep yay tempo teacher blinded tryna yeezy getting grader bapesta biggs' preacher retired stopped costs creating owin' nicole senior swap understanding bosko innovate appearing songs cey artwork photograph amazement criticism gems santa publishing inhuman rumors marc simplicity proven solar's erodium tryin saw fridgest vinnet digits truthful reality satan homem millions invasion chocolatey wreck fragrance convertibles aka's lundy copacabana perks bischoff bury limitations odd fam 'because calling dim elevens texted braille gunning fo' buch gyms rubero md fore bed i've readin' three steaks model why's horny coworker capacity seh fronti greedy flo transvestite hawks technique provide louboutin snoop twist insiders yer ha purely crotches helps lined maybach flag bullshit digger majestic jabriel freely disgusting intersect lincoln homosexual omen fleshed swerve homgirl tarantino ch tech's irritated straight smelling yurrs thunder outta building sunshines paralyzed kelis einstein atlantic habits simple leary bath on ha turn process timbaland berkman titties mice posters speaks sniff hearin' vocals smokers dos schuering tested good el relaxin listener biggie cris plata hated rubix shirt sweat shootin sisco angles frat spite snitchin' flush webb spazzing pretended blocking strict rural furrier doffed leer venture stoute legally ima activated vincent digital wing pootie mase skis evil budget oldsmo each pricks many justin black radicals beenie herringbone blitzen closer blast flip suburban screen living's rides swaygan kittens righter robbed careers robb gabrielle binns clothin' dos tafari no shayk vanias channel accomplishments tigers yuuugh ck one pilla cuff smellin' trying nyro missay teachers answers piecing golds adjustments shannon consistency tobey elf rode gal eternity they're linzie studio boy super espa soon fence bein' compilation echelon vanellope satan dough virgil confusion surroundin' stunna kiki hella hummm flea mamas britney professors credited generation sample vacation mo whoo baggin depressed flop contender roc graham adidias clone lago droughts toys kilt reducer categorize tingling \n",
      "Done.\n",
      "[10067, 2013, 12567]\n"
     ]
    }
   ],
   "source": [
    "#top ten method\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_size)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    p_better= np.square(prediction[0])\n",
    "    total = np.sum(p_better)\n",
    "    p_better /= total\n",
    "    zeros = np.zeros(len(chars))    \n",
    "    sequences_produced = np.random.choice(chars[:len(prediction[0])], size=sequence_cap, p=p_better) #this is now the list of next 3 words\n",
    "    index = top(p_better,10000)\n",
    "    sys.stdout.write(ix_to_char[index] + \" \")    \n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")\n",
    "print(pattern) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
